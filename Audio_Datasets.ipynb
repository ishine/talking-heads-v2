{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Audio_Datasets.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1CnPpyu-Orv9nES6FP-O5FqhMQ9jbksFd",
      "authorship_tag": "ABX9TyPJFA12Dy//ATA75VwP0E/U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/michalis-theodosiou/talking-heads-v2/blob/main/Audio_Datasets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqFliE1In-fs"
      },
      "source": [
        "Clone repo and extract audio files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Ii_Zu-YofS-",
        "outputId": "f5636d6e-2f6a-4c5e-f8a3-8f7881e5b523"
      },
      "source": [
        "!git clone https://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@github.com/michalis-theodosiou/talking-heads-v2.git\n",
        "%cd talking-heads-v2/\n",
        "!pip install -q -r requirements.txt"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'talking-heads-v2'...\n",
            "remote: Enumerating objects: 16, done.\u001b[K\n",
            "remote: Counting objects: 100% (16/16), done.\u001b[K\n",
            "remote: Compressing objects: 100% (11/11), done.\u001b[K\n",
            "remote: Total 16 (delta 1), reused 16 (delta 1), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (16/16), done.\n",
            "/content/talking-heads-v2/talking-heads-v2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XdmyAmg86pQ"
      },
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile(\"/content/drive/MyDrive/Colab Datasets/MEAD_AUDIO_3.zip\",\"r\") as zip_ref:\n",
        "    zip_ref.extractall(\"/content/talking-heads-v2\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elHTlaAWoEk9"
      },
      "source": [
        "Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQIwOxWgrd1X"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import glob\n",
        "from resemblyzer import preprocess_wav, VoiceEncoder\n",
        "import librosa\n",
        "import warnings\n",
        "import random\n",
        "#import concurrent.futures\n",
        "from third_party.ge2e import GE2ELoss"
      ],
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85hDUH2yoJBg"
      },
      "source": [
        "Declare Dataset Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWwQGtcsuh4A"
      },
      "source": [
        "class audio_data_single(Dataset):\n",
        "    def __init__(self, directory):\n",
        "        self.dir = directory\n",
        "        self.filelist = glob.glob('{}/**/*.m4a'.format(self.dir),recursive=True)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.filelist)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        audio_path = self.filelist[idx]\n",
        "        #resamples, normalises vol and trims silences\n",
        "        audio = preprocess_wav(audio_path)\n",
        "\n",
        "        return audio\n",
        "\n",
        "\n",
        "class audio_data_ge2e(Dataset):\n",
        "    \"\"\"\n",
        "    A class to load a batch of audio files to calculate the ge2e loss.\n",
        "    Creates returns data in the format \n",
        "\n",
        "    Input Params\n",
        "    -----------\n",
        "    directory : str\n",
        "      The directory where the audio files are located in the original MEAD directory structure\n",
        "    intensity: int\n",
        "      The intensity (from 1 - 3) from which to load audio\n",
        "    num_utterances: int\n",
        "      The number of utterances to sample from per emotion and speaker\n",
        "\n",
        "    Methods\n",
        "    ----------\n",
        "    __get_item__(idx) : returns dict\n",
        "      Returns a dictionary of lists for the idx speaker in the format {emotion_name:[list of randomly sampled utterances]}\n",
        "\n",
        "    Todo:\n",
        "    ----------\n",
        "    - Add validation/training split functionality\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, directory, intensity, num_utterances):\n",
        "        self.intensity_level = 'level_' + str(intensity)\n",
        "        self.dir = directory\n",
        "        self.filelist = glob.glob('{}/**/{}/*.m4a'.format(self.dir,self.intensity_level),recursive=True)\n",
        "        self.emotions = sorted(list(set(path.split('/')[3] for path in self.filelist)))\n",
        "        self.speakers = sorted(list(set(path.split('/')[1] for path in self.filelist)))\n",
        "        self.utterances = sorted(list(set(path.split('/')[5].split('.')[0] for path in self.filelist)))\n",
        "        self.num_utterances = num_utterances\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.speakers)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # selects one speaker and takes 16 random utterances for each emotion\n",
        "\n",
        "        output_dict = {}\n",
        "        speaker = self.speakers[idx]\n",
        "\n",
        "        with warnings.catch_warnings():\n",
        "          warnings.simplefilter(\"ignore\")\n",
        "          for emotion in self.emotions:\n",
        "              all_files = glob.glob(f'{self.dir}/{speaker}/audio/{emotion}/{self.intensity_level}/*.m4a')\n",
        "              chosen_files = random.sample(all_files,self.num_utterances)\n",
        "          #resamples, normalises vol and trims silences\n",
        "              # with concurrent.futures.ProcessPoolExecutor() as executor:\n",
        "              #   result = executor.map(preprocess_wav,chosen_files)\n",
        "              #   output_dict[emotion] = result\n",
        "\n",
        "              output_dict[emotion] = []\n",
        "              for f in chosen_files:\n",
        "                output = preprocess_wav(f)\n",
        "                output_dict[emotion].append(output)\n",
        "\n",
        "          return output_dict\n",
        "\n",
        "train_dataset = audio_data_ge2e('MEAD_AUDIO_3',intensity=3,num_utterances=16)\n",
        "dataloader = DataLoader(train_dataset,batch_size=1,shuffle=True)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UN7kPd_s6WJ"
      },
      "source": [
        "Todo list:\n",
        "\n",
        "- write training pass function for rezemblyser to output processed "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pSI5sw122p-1",
        "outputId": "f64363fb-0303-426d-e9ce-a5aee4eed5b0"
      },
      "source": [
        "t = ge2e_test_extraction(dataset,0)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded the voice encoder model on cpu in 0.01 seconds.\n",
            "loading data...\n",
            "data loaded, extracting embeddings...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8d4qKM3yYJb"
      },
      "source": [
        "def ge2e_test_extraction(dataset,idx):\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  resemblyzer_encoder = VoiceEncoder(device=device)\n",
        "\n",
        "  print('loading data...')\n",
        "  data = dataset[idx]\n",
        "\n",
        "  print('data loaded, extracting embeddings...')\n",
        "  N = len(dataset.emotions)\n",
        "  M = len(dataset.utterances)\n",
        "  D = 256\n",
        "  output = np.empty([N,M,D],dtype=np.float32)\n",
        "\n",
        "  for n,emotion in enumerate(dataset.emotions):\n",
        "    for m,audio in enumerate(data[emotion]):\n",
        "      output[n,m,:] = resemblyzer_encoder.embed_utterance(audio)\n",
        "  \n",
        "  return output"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GmlD_NRY0KQc"
      },
      "source": [
        "\n",
        "loss_fn = GE2ELoss(init_w=10.0, init_b=-5.0, loss_method='contrast')\n",
        "loss = loss_fn(torch.from_numpy(t))"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmFhnqTOPBoi"
      },
      "source": [
        "class VoiceEncoder_train(VoiceEncoder):\n",
        "\n",
        "  \"\"\"parent class of resemblyzer voice encoder to add embedding function with gradient\"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__(device)\n",
        "\n",
        "  def embed_utterance_train(self,wav,rate=1.3, min_coverage=0.75):\n",
        "    from resemblyzer import audio\n",
        "    wav_slices, mel_slices = self.compute_partial_slices(len(wav), rate, min_coverage)\n",
        "    max_wave_length = wav_slices[-1].stop\n",
        "    if max_wave_length >= len(wav):\n",
        "        wav = np.pad(wav, (0, max_wave_length - len(wav)), \"constant\")\n",
        "    \n",
        "    # Split the utterance into partials\n",
        "    mel = audio.wav_to_mel_spectrogram(wav)\n",
        "    mels = np.array([mel[s] for s in mel_slices])\n",
        "    mels = torch.from_numpy(mels).to(self.device)\n",
        "    # forward through the network\n",
        "    partial_embeds = self(mels)\n",
        "\n",
        "    # Compute the utterance embedding from the partial embeddings\n",
        "    raw_embed = partial_embeds.mean(axis=0)\n",
        "    embed = torch.nn.functional.normalize(raw_embed,p=2,dim=0)\n",
        "\n",
        "    return embed"
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpnzmXZZfjSN"
      },
      "source": [
        "def ge2e_forward(data):\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  encoder = VoiceEncoder_train()\n",
        "\n",
        "  N = len(data.keys())\n",
        "  M = len(data[list(data.keys())[0]])\n",
        "  D = 256\n",
        "  output = torch.empty([N,M,D])\n",
        "\n",
        "  for n,emotion in enumerate(dataset.emotions):\n",
        "    for m,audio in enumerate(data[emotion]):\n",
        "      output[n,m,:] = encoder.embed_utterance_train(audio)\n",
        "  \n",
        "  return output"
      ],
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_otbzUpftA0",
        "outputId": "29ba555e-2864-4587-eef6-11ecef000138"
      },
      "source": [
        "ge2e_forward(data)"
      ],
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded the voice encoder model on cpu in 0.01 seconds.\n",
            "data loaded, extracting embeddings...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0154, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "         ...,\n",
              "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "         [0.1709, 0.0000, 0.0049,  ..., 0.0000, 0.0271, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
              "\n",
              "        [[0.0119, 0.0000, 0.0000,  ..., 0.0000, 0.0061, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0028, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "         ...,\n",
              "         [0.0000, 0.0414, 0.0000,  ..., 0.0000, 0.0000, 0.0034],\n",
              "         [0.0097, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
              "\n",
              "        [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0159],\n",
              "         [0.0000, 0.0018, 0.0000,  ..., 0.0000, 0.0027, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "         ...,\n",
              "         [0.0327, 0.0000, 0.0000,  ..., 0.0000, 0.0051, 0.0000],\n",
              "         [0.0000, 0.0096, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[0.0000, 0.0135, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "         [0.0205, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "         ...,\n",
              "         [0.0041, 0.0144, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
              "\n",
              "        [[0.0181, 0.0000, 0.0000,  ..., 0.0000, 0.0045, 0.0000],\n",
              "         [0.0411, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "         ...,\n",
              "         [0.0320, 0.0000, 0.0000,  ..., 0.0000, 0.0351, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0791, 0.0000],\n",
              "         [0.0415, 0.0000, 0.0075,  ..., 0.0000, 0.0899, 0.0000]],\n",
              "\n",
              "        [[0.0000, 0.0695, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "         [0.0329, 0.0720, 0.0000,  ..., 0.0000, 0.0000, 0.0027],\n",
              "         [0.0168, 0.0927, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "         ...,\n",
              "         [0.0000, 0.0103, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "         [0.0148, 0.0037, 0.0000,  ..., 0.0000, 0.0010, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
              "       grad_fn=<CopySlices>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 160
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9af5b_YjPm8i",
        "outputId": "c05ca946-17a8-40d9-a315-71c777b424b1"
      },
      "source": [
        "tt = VoiceEncoder_train()"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded the voice encoder model on cpu in 0.02 seconds.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScegSdXrYdjC"
      },
      "source": [
        "Training pass of the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sh3PWV3nY0YI",
        "outputId": "2763697a-bf9e-4594-eef5-a9ae90042ebb"
      },
      "source": [
        "# define optimizer\n",
        "# define criterion\n",
        "model = VoiceEncoder_train()\n",
        "criterion = GE2ELoss(init_w=10.0, init_b=-5.0, loss_method='contrast')\n",
        "optimizer = torch.optim.Adam(list(model.parameters()) + list(criterion.parameters()), lr=0.0001)\n",
        "train_dataset = audio_data_ge2e('MEAD_AUDIO_3',intensity=3,num_utterances=16)\n",
        "\n",
        "for epoch in range(1):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "\n",
        "    #shuffle indices\n",
        "    indices = list(range(len(train_dataset)))\n",
        "    random.shuffle(indices)\n",
        "\n",
        "    for i, idx in enumerate(indices): #enumerate over indices\n",
        "        # get the data\n",
        "        data = train_dataset[idx]\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = ge2e_forward(data)\n",
        "        loss = criterion(outputs)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        print(f'epoch {epoch}, batch {i}, loss = {loss.item()}')\n",
        "\n",
        "print('Finished Training')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded the voice encoder model on cpu in 0.02 seconds.\n",
            "Loaded the voice encoder model on cpu in 0.02 seconds.\n",
            "data loaded, extracting embeddings...\n",
            "epoch 0, batch 0, loss = 111.4823226928711\n",
            "Loaded the voice encoder model on cpu in 0.01 seconds.\n",
            "data loaded, extracting embeddings...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqNHl-i9aPvz"
      },
      "source": [
        "indices = list(range(len(train_dataset)))\n",
        "random.shuffle(indices)"
      ],
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0RTwrB_aSrM",
        "outputId": "4fabaaa3-4697-4efd-897c-db78967422c2"
      },
      "source": [
        "indices"
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[47,\n",
              " 25,\n",
              " 1,\n",
              " 27,\n",
              " 14,\n",
              " 22,\n",
              " 4,\n",
              " 29,\n",
              " 40,\n",
              " 32,\n",
              " 18,\n",
              " 11,\n",
              " 26,\n",
              " 12,\n",
              " 43,\n",
              " 42,\n",
              " 23,\n",
              " 35,\n",
              " 19,\n",
              " 7,\n",
              " 5,\n",
              " 17,\n",
              " 45,\n",
              " 34,\n",
              " 10,\n",
              " 3,\n",
              " 13,\n",
              " 37,\n",
              " 41,\n",
              " 36,\n",
              " 38,\n",
              " 39,\n",
              " 31,\n",
              " 0,\n",
              " 33,\n",
              " 24,\n",
              " 9,\n",
              " 6,\n",
              " 8,\n",
              " 30,\n",
              " 46,\n",
              " 15,\n",
              " 20,\n",
              " 21,\n",
              " 44,\n",
              " 2,\n",
              " 16,\n",
              " 28]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSqxT-cHaWhz",
        "outputId": "30220a09-a0f6-4a1c-9e64-5fc73666e059"
      },
      "source": [
        "len(dataset)"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "48"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnkImYsndauY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}